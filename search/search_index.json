{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Better analyze information, in all its forms </p> <p></p>"},{"location":"#implement-your-own-datashare-tasks-written-in-python","title":"Implement your own Datashare tasks, written in Python","text":"<p>Most AI, Machine Learning, Data Engineering happens in Python. Datashare now lets you extend its backend with your own tasks implemented in Python.</p> <p>Turning your own ML pipelines into Datashare tasks is very simple.</p> <p>Actually, it's almost as simple as cloning our template repo:</p> git clone git@github.com:ICIJ/datashare-python.git <p>replacing existing app tasks with your own:  <pre><code>from icij_worker import AsyncApp\n\napp = AsyncApp(\"some-app\")\n\n\n@app.task\ndef hello_world() -&gt; str:\n    return \"Hello world\"\n</code></pre></p> <p>installing <code>uv</code> to set up dependencies and running your async Datashare worker:</p> cd datashare-pythoncurl -LsSf https://astral.sh/uv/install.sh | shuv run ./scripts/worker_entrypoint.sh[INFO][icij_worker.backend.backend]: Loading worker configuration from env......}[INFO][icij_worker.backend.mp]: starting 1 worker for app datashare_python.app.app... <p>you'll then be able to execute task by starting using our HTTP client (and soon using Datashare's UI).    </p>"},{"location":"#learn","title":"Learn","text":"<p>Learn how to integrate Data Processing and Machine Learning pipelines to Datashare following our tutorial. </p>"},{"location":"#get-started","title":"Get started","text":"<p>Follow our get started guide an learn how to clone the template repository and implement your own Datashare tasks !</p>"},{"location":"#refine-your-knowledge","title":"Refine your knowledge","text":"<p>Follow our guides to learn how to implement complex tasks and deploy Datashare workers running your own tasks.</p>"},{"location":"faq/","title":"Frequently Asked Questions (WIP)","text":""},{"location":"get-started/","title":"About","text":"<p>This section will is a step-by-step guide to create and deploy your own Datashare tasks.</p> <p>You might want to learn the basics before actually starting to implement your own worker.</p>"},{"location":"get-started/build/","title":"Build your worker image","text":""},{"location":"get-started/build/#local-docker-build","title":"Local docker build","text":"<p>To build a docker image of the worker we've previously built, we can use the  <code>datashare-python</code> script.</p> <p>It's a tiny wrapper around the <code>docker compose</code> CLI:</p> ./datashare-python build datashare-python=&gt; [datashare-python internal] booting buildkit                                                                                                                                                                                        0.9s=&gt; =&gt; starting container buildx_buildkit_strange_curran0                                                                                                                                                                        0.9s=&gt; [datashare-python internal] load build definition from Dockerfile...=&gt; =&gt; exporting layers                                                                                                                                                                                                         56.1s=&gt; =&gt; exporting manifest sha256:7462a3f43df6073c57fc2482726a65d43e4f83f68ccd098ec0804b8b959d9a17                                                                                                                                0.0s=&gt; =&gt; exporting config sha256:184ed641f1ee82d4eb068143702d3cbec32b25413051764000353b02458e12a1                                                                                                                                  0.0s=&gt; =&gt; sending tarball                                                                                                                                                                                                          38.7s=&gt; [datashare-python datashare-python] importing to docker"},{"location":"get-started/build/#publish-to-docker-hub","title":"Publish to Docker Hub","text":"<p>You can also fork the template repository and use the CI to build and publish the worker image to Docker Hub.</p> <p>To publish, make sure to set the <code>DOCKERHUB_USERNAME</code> and <code>DOCKERHUB_TOKEN</code> (see documentation) and then just create a tag to trigger the build.</p>"},{"location":"get-started/run/","title":"Deploy using Docker","text":""},{"location":"get-started/run/#running-locally-with-docker-compose","title":"Running locally with Docker Compose","text":"<p>To run your worker together with Datashare, you can use the <code>datashare-python</code> <code>docker compose</code> CLI wrapper:</p> ./datashare-python up"},{"location":"get-started/run/#deploying-to-a-server","title":"Deploying to a server","text":"<p>To deploy on a server, just use the image we've previously published to Docker Hub.</p>"},{"location":"get-started/start-tasks/","title":"Start tasks","text":"<p>Currently, it's you can interact with Datashare tasks using the <code>/api/task/*</code> HTTP APIs route of Datashare.</p> <p>With Datashare and your worker running, you can now use the <code>datashare-python</code> CLI for that:</p> uvx datashare-python tasks --help"},{"location":"get-started/start-tasks/#basic-worker","title":"Basic worker","text":"<p>Let's create a task, monitor it and get its result:</p> uvx datashare-python tasks create hello_worlduvx datashare-python tasks watch  &lt;&gt;uvx datashare-python tasks result  &lt;&gt;"},{"location":"get-started/start-tasks/#advanced-worker","title":"Advanced worker","text":"<p>First let's create</p> uvx datashare-python tasks create hello_worlduvx datashare-python tasks watch  &lt;&gt;uvx datashare-python tasks result  &lt;&gt;"},{"location":"get-started/implement/","title":"How to use the worker template repo ?","text":"<p>The datashare-python repository is meant to be used as a template to implement your own Datashare worker.</p>"},{"location":"get-started/implement/#clone-the-template-repository","title":"Clone the template repository","text":"<p>Start by cloning the template repository:</p> git clone git@github.com:ICIJ/datashare-python.git"},{"location":"get-started/implement/#explore-the-codebase","title":"Explore the codebase","text":"<p>In addition to be used as a template, the repository can also showcases some of advanced schemes detailed in the guides section of this documentation.</p> <p>Don't hesitate to have a look at the codebase before starting (or get back to it later on) !</p> <p>In particular the following files should be of interest: <pre><code>.\n\u251c\u2500\u2500 datashare_python\n\u2502         \u251c\u2500\u2500 app.py\n\u2502         \u251c\u2500\u2500 config.py\n\u2502         \u251c\u2500\u2500 tasks\n\u2502         \u2502         \u251c\u2500\u2500 __init__.py\n\u2502         \u2502         \u251c\u2500\u2500 classify_docs.py\n\u2502         \u2502         \u251c\u2500\u2500 dependencies.py\n\u2502         \u2502         \u2514\u2500\u2500 translate_docs.py\n</code></pre></p>"},{"location":"get-started/implement/#replace-existing-tasks-with-your-own","title":"Replace existing tasks with your own","text":"<p>To implement your Datashare worker the only thing you have to do is to replace existing tasks with your own and register them in the <code>app</code> app variable of the <code>app.py</code> file.</p> <p>We'll detail how to do so in the Basic Worker and Advanced Worker examples.</p>"},{"location":"get-started/implement/worker-advanced/","title":"Advanced Datashare worker","text":"<p>In this section we'll augment the worker template app (translation and classification) with  vector store to allow us to perform semantic similarity searches between queries and Datashare docs.</p> <p>Make sure you've followed the basic worker example to understand the basics ! </p>"},{"location":"get-started/implement/worker-advanced/#clone-the-template-repository","title":"Clone the template repository","text":"<p>Start over and clone the template repository once again:</p> git clone git@github.com:ICIJ/datashare-python.git"},{"location":"get-started/implement/worker-advanced/#install-extra-dependencies","title":"Install extra dependencies","text":"<p>We'll use LanceDB to implement our vector store, we need to add it as well as the sentence-transformers to our dependencies: </p> uv add lancedb sentence-transformers <p>Note</p> <p>In a production setup, since elasticsearch implements its own vector database it might have been convenient to use it. For this examples, we're using LanceDB as it's embedded and doesn't require any deployment update.</p>"},{"location":"get-started/implement/worker-advanced/#embedding-datashare-documents","title":"Embedding Datashare documents","text":"<p>For the demo purpose, we'll split the task of embedding docs into two tasks:</p> <ul> <li>the <code>create_vectorization_tasks</code> which scans the index, get IDs of Datashare docs and batch them and create <code>vectorize_docs</code> tasks</li> <li>the <code>vectorize_docs</code> tasks (triggered by the <code>create_vectorization_tasks</code> task) receives docs IDs,  fetch the doc contents from the index and add them to vector database</li> </ul> <p>Note</p> <p>We could have performed vectorization in a single task, having first task splitting a large tasks into batches/chunks is a commonly used pattern to distribute heavy workloads across workers (learn more in the task workflow guide).</p>"},{"location":"get-started/implement/worker-advanced/#the-create_vectorization_tasks-task","title":"The <code>create_vectorization_tasks</code> task","text":"<p>The <code>create_vectorization_tasks</code> is defined in the <code>tasks/vectorize.py</code> file as following: tasks/vectorize.py<pre><code>import asyncio\nimport logging\nfrom collections.abc import AsyncIterable\nfrom typing import AsyncIterator\n\nimport numpy as np\nfrom icij_common.es import (\n    DOC_CONTENT,\n    ESClient,\n    HITS,\n    ID_,\n    QUERY,\n    SOURCE,\n    ids_query,\n    make_document_query,\n    match_all,\n)\nfrom icij_worker.ds_task_client import DatashareTaskClient\nfrom lancedb import AsyncConnection as LanceDBConnection, AsyncTable\nfrom lancedb.embeddings import get_registry\nfrom lancedb.index import FTS, IvfPq\nfrom lancedb.pydantic import LanceModel, Vector\nfrom sentence_transformers import SentenceTransformer\n\nfrom datashare_python.constants import PYTHON_TASK_GROUP\nfrom datashare_python.tasks.dependencies import (\n    lifespan_es_client,\n    lifespan_task_client,\n    lifespan_vector_db,\n)\nfrom datashare_python.utils import async_batches\n\nlogger = logging.getLogger(__name__)\n\n\nasync def recreate_vector_table(\n    vector_db: LanceDBConnection, schema: type[LanceModel]\n) -&gt; AsyncTable:\n    table_name = \"ds_docs\"\n    existing_tables = await vector_db.table_names()\n    if table_name in existing_tables:\n        logging.info(\"deleting previous vector db...\")\n        await vector_db.drop_table(table_name)\n    table = await vector_db.create_table(table_name, schema=schema)\n    return table\n\n\ndef make_record_schema(model: str) -&gt; type[LanceModel]:\n    model = get_registry().get(\"huggingface\").create(name=model)\n\n    class RecordSchema(LanceModel):\n        doc_id: str\n        content: str = model.SourceField()\n        vector: Vector(model.ndims()) = model.VectorField()\n\n    return RecordSchema\n\n\nasync def create_vectorization_tasks(\n    project: str,\n    *,\n    model: str = \"BAAI/bge-small-en-v1.5\",\n    es_client: ESClient | None = None,\n    task_client: DatashareTaskClient | None = None,\n    vector_db: LanceDBConnection | None = None,\n    batch_size: int = 16,\n) -&gt; list[str]:\n    if es_client is None:\n        es_client = lifespan_es_client()\n    if task_client is None:\n        task_client = lifespan_task_client()\n    if vector_db is None:\n        vector_db = lifespan_vector_db()\n    schema = make_record_schema(model)\n    await recreate_vector_table(vector_db, schema)\n    query = make_document_query(match_all())\n    docs_pages = es_client.poll_search_pages(\n        index=project, body=query, sort=\"_doc:asc\", _source=False\n    )\n    doc_ids = (doc[ID_] async for doc in _flatten_search_pages(docs_pages))\n    batches = async_batches(doc_ids, batch_size=batch_size)\n    logging.info(\"spawning vectorization tasks...\")\n    args = {\"project\": project}\n    task_ids = []\n    async for batch in batches:\n        args[\"docs\"] = list(batch)\n        task_id = await task_client.create_task(\n            \"vectorize_docs\", args, group=PYTHON_TASK_GROUP.name\n        )\n        task_ids.append(task_id)\n    logging.info(\"created %s vectorization tasks !\", len(task_ids))\n    return task_ids\n\n\nasync def _flatten_search_pages(pages: AsyncIterable[dict]) -&gt; AsyncIterator[dict]:\n    async for page in pages:\n        for doc in page[HITS][HITS]:\n            yield doc\n</code></pre></p> <p>The function starts by creating a schema for our vector DB table using the convenient LanceDB embedding function feature, which will automatically create the record <code>vector field from the provided source field (</code>content` in our case) using our HuggingFace embedding model: tasks/vectorize.py<pre><code>def make_record_schema(model: str) -&gt; type[LanceModel]:\n    model = get_registry().get(\"huggingface\").create(name=model)\n\n    class RecordSchema(LanceModel):\n        doc_id: str\n        content: str = model.SourceField()\n        vector: Vector(model.ndims()) = model.VectorField()\n\n    return RecordSchema\n</code></pre></p> <p>We then (re)-create a vector table using the DB connection provided by dependency injection (see the next section to learn more): tasks/vectorize.py<pre><code>async def recreate_vector_table(\n    vector_db: LanceDBConnection, schema: type[LanceModel]\n) -&gt; AsyncTable:\n    table_name = \"ds_docs\"\n    existing_tables = await vector_db.table_names()\n    if table_name in existing_tables:\n        logging.info(\"deleting previous vector db...\")\n        await vector_db.drop_table(table_name)\n    table = await vector_db.create_table(table_name, schema=schema)\n    return table\n</code></pre></p> <p>Next <code>create_vectorization_tasks</code> queries the index matching all documents: tasks/vectorize.py<pre><code>    query = make_document_query(match_all())\n</code></pre> and scroll through results pages creating batches of <code>batch_size</code>: tasks/vectorize.py<pre><code>    docs_pages = es_client.poll_search_pages(\n        index=project, body=query, sort=\"_doc:asc\", _source=False\n    )\n    doc_ids = (doc[ID_] async for doc in _flatten_search_pages(docs_pages))\n    batches = async_batches(doc_ids, batch_size=batch_size)\n</code></pre></p> <p>Finally, for each batch, it spawns a vectorization task using the datashare task client and returns the list of created tasks: tasks/vectorize.py<pre><code>    args = {\"project\": project}\n    task_ids = []\n    async for batch in batches:\n        args[\"docs\"] = list(batch)\n        task_id = await task_client.create_task(\n            \"vectorize_docs\", args, group=PYTHON_TASK_GROUP.name\n        )\n        task_ids.append(task_id)\n    logging.info(\"created %s vectorization tasks !\", len(task_ids))\n    return task_ids\n</code></pre></p>"},{"location":"get-started/implement/worker-advanced/#the-lifespan_vector_db-dependency-injection","title":"The <code>lifespan_vector_db</code> dependency injection","text":"<p>In order to avoid to re-create a DB connection each time the worker processes a task, we leverage  dependency injection in order to create the connection at start up and retrieve it inside our function.</p> <p>This pattern is already used for the elasticsearch client and the datashare task client, to use it for the vector DB connection, we'll need to update the dependencies.py file.</p> <p>First we need to implement the dependency setup function: dependencies.py<pre><code>from lancedb import AsyncConnection as LanceDBConnection, connect_async\n\nfrom datashare_python.constants import DATA_DIR\n\n_VECTOR_DB_CONNECTION: LanceDBConnection | None = None\n_DB_PATH = DATA_DIR / \"vector.db\"\n\n\nasync def vector_db_setup(**_):\n    global _VECTOR_DB_CONNECTION\n    _VECTOR_DB_CONNECTION = await connect_async(_DB_PATH)\n</code></pre></p> <p>The function creates a connection to the vector DB located on the filesystem and stores the connection to a global variable.</p> <p>We then have to implement a function to make this global available to the rest of the codebase: dependencies.py<pre><code>def lifespan_vector_db() -&gt; LanceDBConnection:\n    if _VECTOR_DB_CONNECTION is None:\n        raise DependencyInjectionError(\"vector db connection\")\n    return _VECTOR_DB_CONNECTION\n</code></pre> We also need to make sure the connection is properly exited when the worker stops by implementing the dependency tear down. We just call the <code>AsyncConnection.__aexit__</code> methode: dependencies.py<pre><code>async def vector_db_teardown(exc_type, exc_val, exc_tb):\n    await lifespan_vector_db().__aexit__(exc_type, exc_val, exc_tb)\n    global _VECTOR_DB_CONNECTION\n    _VECTOR_DB_CONNECTION = None\n</code></pre></p> <p>Read the dependency injection guide to learn more !</p>"},{"location":"get-started/implement/worker-advanced/#the-vectorize_docs-task","title":"The <code>vectorize_docs</code> task","text":"<p>Next we implement the <code>vectorize_docs</code> as following:</p> tasks/vectorize.py<pre><code>async def vectorize_docs(\n    docs: list[str],\n    project: str,\n    *,\n    es_client: ESClient | None = None,\n    vector_db: LanceDBConnection | None = None,\n) -&gt; int:\n    if es_client is None:\n        es_client = lifespan_es_client()\n    if vector_db is None:\n        vector_db = lifespan_vector_db()\n    n_docs = len(docs)\n    logging.info(\"vectorizing %s docs...\", n_docs)\n    query = {QUERY: ids_query(docs)}\n    docs_pages = es_client.poll_search_pages(\n        index=project, body=query, sort=\"_doc:asc\", _source_includes=[DOC_CONTENT]\n    )\n    es_docs = _flatten_search_pages(docs_pages)\n    table = await vector_db.open_table(\"ds_docs\")\n    records = [\n        {\"doc_id\": d[ID_], \"content\": d[SOURCE][DOC_CONTENT]} async for d in es_docs\n    ]\n    await table.add(records)\n    logging.info(\"vectorized %s docs !\", n_docs)\n    return n_docs\n</code></pre> <p>The task function starts by retriving the batch document contents, querying the index by doc IDs: tasks/vectorize.py<pre><code>    query = {QUERY: ids_query(docs)}\n    docs_pages = es_client.poll_search_pages(\n        index=project, body=query, sort=\"_doc:asc\", _source_includes=[DOC_CONTENT]\n    )\n    es_docs = _flatten_search_pages(docs_pages)\n</code></pre></p> <p>Finally, we add each doc content to the vector DB table, because we created table using a schema and the embedding function feature, the embedding vector  will be automatically created from the <code>content</code> source field: tasks/vectorize.py<pre><code>    table = await vector_db.open_table(\"ds_docs\")\n    records = [\n        {\"doc_id\": d[ID_], \"content\": d[SOURCE][DOC_CONTENT]} async for d in es_docs\n    ]\n    await table.add(records)\n</code></pre></p>"},{"location":"get-started/implement/worker-advanced/#semantic-similarity-search","title":"Semantic similarity search","text":"<p>Now that we've built a vector store from Datashare's docs, we need to query it. Let's create a <code>find_most_similar</code> task which find the most similar docs for a provided set of queries.</p> <p>The task function starts by loading the embedding model and vectorizes the input queries: </p> tasks/vectorize.py<pre><code>async def find_most_similar(\n    queries: list[str],\n    model: str,\n    *,\n    vector_db: LanceDBConnection | None = None,\n    n_similar: int = 2,\n) -&gt; list[list[dict]]:\n    if vector_db is None:\n        vector_db = lifespan_vector_db()\n    n_queries = len(queries)\n    logging.info(\"performing similarity search for %s queries...\", n_queries)\n    table = await vector_db.open_table(\"ds_docs\")\n    # Create indexes for hybrid search\n    try:\n        await table.create_index(\n            \"vector\", config=IvfPq(distance_type=\"cosine\"), replace=False\n        )\n        await table.create_index(\"content\", config=FTS(), replace=False)\n    except RuntimeError:\n        logging.debug(\"skipping index creation as they already exist\")\n    vectorizer = SentenceTransformer(model)\n    vectors = vectorizer.encode(queries)\n    futures = (\n        _find_most_similar(table, q, v, n_similar) for q, v in zip(queries, vectors)\n    )\n    results = await asyncio.gather(*futures)\n    results = sum(results, start=[])\n    logging.info(\"completed similarity search for %s queries !\", n_queries)\n    return results\n\n\nasync def _find_most_similar(\n    table: AsyncTable, query: str, vector: np.ndarray, n_similar: int\n) -&gt; list[dict]:\n    # pylint: disable=unused-argument\n    most_similar = (\n        await table.query()\n        # The async client seems to be bugged and does really support hybrid queries\n        # .nearest_to_text(query, columns=[\"content\"])\n        .nearest_to(vector)\n        .limit(n_similar)\n        .select([\"doc_id\"])\n        .to_list()\n    )\n    most_similar = [\n        {\"doc_id\": s[\"doc_id\"], \"distance\": s[\"_distance\"]} for s in most_similar\n    ]\n    return most_similar\n</code></pre> <p>it then performs an hybrid search, using both the input query vector and its text:</p> tasks/vectorize.py<pre><code>async def _find_most_similar(\n    table: AsyncTable, query: str, vector: np.ndarray, n_similar: int\n) -&gt; list[dict]:\n    # pylint: disable=unused-argument\n    most_similar = (\n        await table.query()\n        # The async client seems to be bugged and does really support hybrid queries\n        # .nearest_to_text(query, columns=[\"content\"])\n        .nearest_to(vector)\n        .limit(n_similar)\n        .select([\"doc_id\"])\n        .to_list()\n    )\n    most_similar = [\n        {\"doc_id\": s[\"doc_id\"], \"distance\": s[\"_distance\"]} for s in most_similar\n    ]\n    return most_similar\n</code></pre>"},{"location":"get-started/implement/worker-advanced/#registering-the-new-tasks","title":"Registering the new tasks","text":"<p>In order to turn our function into a Datashare task, we have to register it into the  <code>app</code> async app variable of the app.py file, using the <code>@task</code> decorator:</p> app.py<pre><code>from typing import Optional\n\nfrom icij_worker import AsyncApp\nfrom icij_worker.typing_ import PercentProgress\nfrom pydantic import parse_obj_as\n\nfrom datashare_python.constants import PYTHON_TASK_GROUP\nfrom datashare_python.objects import ClassificationConfig, TranslationConfig\nfrom datashare_python.tasks import (\n    classify_docs as classify_docs_,\n    create_classification_tasks as create_classification_tasks_,\n    create_translation_tasks as create_translation_tasks_,\n    translate_docs as translate_docs_,\n)\nfrom datashare_python.tasks.dependencies import APP_LIFESPAN_DEPS\nfrom datashare_python.tasks.vectorize import (\n    create_vectorization_tasks as create_vectorization_tasks_,\n    find_most_similar as find_most_similar_,\n    vectorize_docs as vectorization_docs_,\n)\n\napp = AsyncApp(\"ml\", dependencies=APP_LIFESPAN_DEPS)\n\n\n@app.task(group=PYTHON_TASK_GROUP)\nasync def create_vectorization_tasks(\n    project: str, model: str = \"BAAI/bge-small-en-v1.5\"\n) -&gt; list[str]:\n    return await create_vectorization_tasks_(project, model=model)\n\n\n@app.task(group=PYTHON_TASK_GROUP)\nasync def vectorization_docs(docs: list[str], project: str) -&gt; int:\n    return await vectorization_docs_(docs, project)\n\n\n@app.task(group=PYTHON_TASK_GROUP)\nasync def find_most_similar(\n    queries: list[str], model: str, n_similar: int = 2\n) -&gt; list[list[dict]]:\n    return await find_most_similar_(queries, model, n_similar=n_similar)\n</code></pre>"},{"location":"get-started/implement/worker-advanced/#testing","title":"Testing","text":"<p>Finally, we implement some tests in the <code>tests/tasks/test_vectorize.py</code> file:</p> tests/tasks/test_vectorize.py<pre><code>from pathlib import Path\nfrom typing import List\n\nimport pytest\nfrom icij_common.es import ESClient\nfrom lancedb import AsyncConnection as LanceDBConnection, connect_async\n\nfrom datashare_python.objects import Document\nfrom datashare_python.tasks.vectorize import (\n    create_vectorization_tasks,\n    find_most_similar,\n    make_record_schema,\n    recreate_vector_table,\n    vectorize_docs,\n)\nfrom datashare_python.tests.conftest import TEST_PROJECT\nfrom datashare_python.utils import DSTaskClient\n\n\n@pytest.fixture\nasync def test_vector_db(tmpdir) -&gt; LanceDBConnection:\n    db = await connect_async(Path(tmpdir) / \"test_vectors.db\")\n    return db\n\n\n@pytest.mark.integration\nasync def test_create_vectorization_tasks(\n    populate_es: List[Document],  # pylint: disable=unused-argument\n    test_es_client: ESClient,\n    test_task_client: DSTaskClient,\n    test_vector_db: LanceDBConnection,\n):\n    # When\n    task_ids = await create_vectorization_tasks(\n        project=TEST_PROJECT,\n        es_client=test_es_client,\n        task_client=test_task_client,\n        vector_db=test_vector_db,\n        batch_size=2,\n    )\n    # Then\n    assert len(task_ids) == 2\n\n\n@pytest.mark.integration\nasync def test_vectorize_docs(\n    populate_es: List[Document],  # pylint: disable=unused-argument\n    test_es_client: ESClient,\n    test_vector_db: LanceDBConnection,\n):\n    # Given\n    model = \"BAAI/bge-small-en-v1.5\"\n    docs = [\"doc-0\", \"doc-3\"]\n    schema = make_record_schema(model)\n    await recreate_vector_table(test_vector_db, schema)\n\n    # When\n    n_vectorized = await vectorize_docs(\n        docs,\n        TEST_PROJECT,\n        es_client=test_es_client,\n        vector_db=test_vector_db,\n    )\n    # Then\n    assert n_vectorized == 2\n    table = await test_vector_db.open_table(\"ds_docs\")\n    records = await table.query().to_list()\n    assert len(records) == 2\n    doc_ids = sorted(d[\"doc_id\"] for d in records)\n    assert doc_ids == [\"doc-0\", \"doc-3\"]\n    assert all(\"vector\" in r for r in records)\n\n\n@pytest.mark.integration\nasync def test_find_most_similar(test_vector_db: LanceDBConnection):\n    # Given\n    model = \"BAAI/bge-small-en-v1.5\"\n    schema = make_record_schema(model)\n    table = await recreate_vector_table(test_vector_db, schema)\n    docs = [\n        {\"doc_id\": \"novel\", \"content\": \"I'm a doc about novels\"},\n        {\"doc_id\": \"monkey\", \"content\": \"I'm speaking about monkeys\"},\n    ]\n    await table.add(docs)\n    queries = [\"doc about books\", \"doc speaking about animal\"]\n\n    # When\n    n_similar = 1\n    most_similar = await find_most_similar(\n        queries, model, vector_db=test_vector_db, n_similar=n_similar\n    )\n    # Then\n    assert len(most_similar) == 2\n    similar_ids = [s[\"doc_id\"] for s in most_similar]\n    assert similar_ids == [\"novel\", \"monkey\"]\n    assert all(\"distance\" in s for s in most_similar)\n</code></pre> <p>We can then run the tests after starting test services using the <code>datashare-python</code> Docker Compose wrapper:</p> ./datashare-python up -d postgresql redis elasticsearch rabbitmq datashare_webuv run --frozen pytest datashare_python/tests/tasks/test_vectorize.py===== test session starts =====collected 3 itemsdatashare_python/tests/tasks/test_vectorize.py ...                                                                                                                                                                                          [100%]====== 3 passed in 6.87s ======...."},{"location":"get-started/implement/worker-advanced/#summary","title":"Summary","text":"<p>We've successfully added a vector store to Datashare !</p> <p>Rather than copy-pasting the above code blocks, you can replace/update your codebase with the following files:</p> <ul> <li><code>datashare_python/tasks/vectorize.py</code></li> <li><code>datashare_python/tasks/dependencies</code></li> <li><code>datashare_python/app.py</code></li> <li>`datashare_python/tests/tasks/test_vectorize.py</li> </ul>"},{"location":"get-started/implement/worker-basic/","title":"Basic Datashare worker","text":""},{"location":"get-started/implement/worker-basic/#install-dependencies","title":"Install dependencies","text":"<p>Start by installing <code>uv</code> and dependencies:</p> curl -LsSf https://astral.sh/uv/install.sh | shuv sync --frozen --group dev"},{"location":"get-started/implement/worker-basic/#implement-the-hello_user-task-function","title":"Implement the <code>hello_user</code> task function","text":"<p>As seen in the task tutorial, one of the dummiest tasks we can implement take the <code>user: dict | None</code> argument automatically added by Datashare to all tasks and greet that user.</p> <p>The function performing this task is the following </p> <pre><code>def hello_user(user: dict | None) -&gt; str:\n    greeting = \"Hello \"\n    if user is None:\n        user = \"unknown\"\n    else:\n        user = user[\"id\"]\n    return greeting + user\n</code></pre>"},{"location":"get-started/implement/worker-basic/#register-the-hello_user-task","title":"Register the <code>hello_user</code> task","text":"<p>In order to turn our function into a Datashare task, we have to register it into the  <code>app</code> async app variable of the app.py file, using the <code>@task</code> decorator.</p> <p>Since we won't use existing tasks, we can also perform some cleaning and get rid of them. The <code>app.py</code> file should hence look like this:</p> app.py<pre><code>from icij_worker import AsyncApp\nfrom icij_worker.app import TaskGroup\n\napp = AsyncApp(\"some-app\")\n\nPYTHON_TASK_GROUP = TaskGroup(name=\"PYTHON\")\n\n\n@app.task(name=\"hello_user\", group=PYTHON_TASK_GROUP)\ndef hello_user(user: dict | None) -&gt; str:\n    greeting = \"Hello \"\n    if user is None:\n        user = \"unknown\"\n    else:\n        user = user[\"id\"]\n    return greeting + user\n</code></pre> <p>The only thing we had to do is to use the <code>@app.task</code> decorator and make sure to provide it with <code>name</code> to bind the function to a task name and group the task in the <code>PYTHON_TASK_GROUP = TaskGroup(name=\"PYTHON\")</code>.</p> <p>As detailed in here, using this task group ensures that when custom tasks are published for execution, they are correctly routed to your custom Python worker and not to the Java built-in workers running behind Datashare's backend. </p>"},{"location":"get-started/implement/worker-basic/#get-rid-of-unused-codebase","title":"Get rid of unused codebase","text":""},{"location":"get-started/implement/worker-basic/#next","title":"Next","text":"<p>Now that you have created a basic app, you can either:</p> <ul> <li>learn how to build a docker image from it</li> <li>learn how to implement a more realistic worker in the advanced example</li> </ul>"},{"location":"guides/","title":"Guides","text":"<ul> <li> <p> Configuration</p> <p>Learn how to configure Datashare, your workers and app to run your own tasks</p> <p> Configuration</p> </li> <li> <p> Deploy with Docker</p> <p>Learn how to deploy your workers using Docker   </p> <p> Deploy with Docker</p> </li> <li> <p> Dependency injection</p> <p>Learn how to add lifespan dependencies to your app: app config, loggers, DB connections, clients...</p> <p> Dependency injection</p> </li> <li> <p> Datashare index reads and writes</p> <p>Learn how to read from and write to Datashare's index     </p> <p> Datashare index reads and writes</p> </li> <li> <p> Task workflows</p> <p>Learn about complex task workflows, batched task, sequential tasks...     </p> <p> Task workflows</p> </li> <li> <p> Task routing</p> <p>Learn distribute specific tasks to workers with the right computational resources    </p> <p> Task routing</p> </li> <li> <p> Failure recovery</p> <p>Learn how to implement robust tasks which can recover from expected failures   </p> <p> Failure recovery</p> </li> <li> <p> Testing</p> <p>Learn how to test your tasks</p> <p> Testing</p> </li> <li> <p> Progress updates</p> <p>Learn how to perform complex progress updates to monitor your tasks   </p> <p> Progress updates</p> </li> </ul>"},{"location":"guides/datashare-index/","title":"Datashare index read and writes (WIP)","text":""},{"location":"guides/dependency-injection/","title":"Dependency injection (WIP)","text":""},{"location":"guides/deploy-with-docker/","title":"Deploy with Docker (WIP)","text":""},{"location":"guides/failure-recovery/","title":"Failure recovery (WIP)","text":""},{"location":"guides/progress-updates/","title":"Progress updates (WIP)","text":""},{"location":"guides/task-routing/","title":"Task routing (WIP)","text":""},{"location":"guides/task-workflows/","title":"Task workflows (WIP)","text":""},{"location":"guides/testing/","title":"Testing (WIP)","text":""},{"location":"guides/config/app-config/","title":"App config (WIP)","text":""},{"location":"guides/config/config-files-and-env-vars/","title":"Config files and env variables (WIP)","text":""},{"location":"guides/config/worker-config/","title":"Worker config (WIP)","text":""},{"location":"learn/","title":"About","text":"<p>This section is a tutorial and will guide you through the basic steps theoretically required for building your own tasks for Datashare from scratch.</p> <p>Don't worry, in practice, you won't have to start anything from scratch. The get started section will show you how to create Datashare tasks by cloning the datashare-python template repo.</p> <p>However, this section will teach you the basic steps it took to build this template.</p>"},{"location":"learn/app/","title":"Building an app featuring multiple tasks","text":"<p>Let's recap, in the previous section, we learnt how to:</p> <ul> <li>transform Python functions into asynchronous tasks</li> <li>register tasks with a name</li> <li>create tasks with arguments</li> <li>create task which can publish progress updates</li> </ul>"},{"location":"learn/app/#full-async-app","title":"Full async app","text":"<p>If we put everything together, we can build a full async app, featuring multiple tasks:</p> my_app.py<pre><code>from icij_worker import AsyncApp\nfrom icij_worker.typing_ import RateProgress\n\napp = AsyncApp(\"some-app\")\n\n\n@app.task(name=\"hello_world\")\ndef hello_world() -&gt; str:\n    return \"Hello world\"\n\n\n@app.task(name=\"hello_user\")\ndef hello_user(user: str | None) -&gt; str:\n    greeting = \"Hello \"\n    if user is None:\n        user = \"unknown\"\n    return greeting + user\n\n\n@app.task(name=\"hello_user_progress\")\nasync def hello_user_progress(user: str | None, progress: RateProgress) -&gt; str:\n    greeting = \"Hello \"\n    await progress(0.5)\n    if user is None:\n        user = \"unknown\"\n    res = greeting + user\n    await progress(1)\n    return res\n</code></pre>"},{"location":"learn/app/#next","title":"Next","text":"<p>Perfect, we've built our first app by registering many async tasks, we need to run pool of async workers to execute these task when asked to do so ! </p>"},{"location":"learn/concepts-advanced/","title":"Advanced concepts and definitions","text":"<p>The following concepts are not required to understand what's following, you get back to them as need. </p>"},{"location":"learn/concepts-advanced/#worker-pools","title":"Worker pools","text":"<p>A worker pool is a collection of task workers running the same async app on the same machine.</p> <p>A worker pool can be created and started using the <code>icij-worker</code> CLI. </p>"},{"location":"learn/concepts-advanced/#broker","title":"Broker","text":"<p>We call broker the messaging service and protocol allowing Datashare's task manager and task workers to communicate together.   </p> <p>Behind the scene we use a custom task protocol built on the top of RabbitMQ's AMQP protocol.</p>"},{"location":"learn/concepts-advanced/#queues","title":"Queues","text":"<p>Queues are stacks of messages sent over the broker. It's possible to route messages to specific queues so that it's read by some specific agents (task manager or some specific worker).</p> <p>While there can be several workers, there is single task manager, and it's already running inside Datashare's backend, so most of the time you don't have to care about it !</p>"},{"location":"learn/concepts-advanced/#task-states","title":"Task States","text":"<p>A task can have different task states: <pre><code>class TaskState(str, Enum):\n    # When created through the TaskManager\n    CREATED = \"CREATED\"\n    # When published to some queue, but not grabbed by a worker yet\n    QUEUED = \"QUEUED\"\n    # When grabbed by and executed by a worker\n    RUNNING = \"RUNNING\"\n    # When the worker couldn't execute the task for some reason\n    ERROR = \"ERROR\"\n    # When successful\n    DONE = \"DONE\"\n    # When cancelled by the user\n    CANCELLED = \"CANCELLED\"\n</code></pre></p>"},{"location":"learn/concepts-advanced/#task-groups","title":"Task Groups","text":"<p>Because its can be convenient to implement task meant to be executed by different types of workers in the same app, tasks can be grouped together to be executed by a given type of worker. </p> <p>In an ML context it's frequent to perform preprocessing tasks which require memory and CPU, and then perform ML inference which require GPU.  Because, it's convenient to implement all these tasks as part of the same codebase (in particular for testing) and register them in the same app, task can be assigned to a group.</p> <p>When starting a worker it's possible to provide a group of tasks to be executed. In the above case, we could for instance define a <code>cpu</code> and a <code>gpu</code> groups and split task between them.</p>"},{"location":"learn/concepts-basic/","title":"Basic concepts and definitions","text":"<p>Before starting, here are a few definitions of concepts that we'll regularly use in this documentation.</p> <p>The following concepts are important for the rest of this tutorial, make sure you understand them properly !</p>"},{"location":"learn/concepts-basic/#definitions","title":"Definitions","text":""},{"location":"learn/concepts-basic/#tasks","title":"Tasks","text":"<p>Tasks (a.k.a. \"async tasks\" or \"asynchronous tasks\") are units of work that can be executed asynchronously.</p> <p>Datashare has its own built-in tasks such as indexing documents, finding named entities, performing search or download by batches... Tasks are visible in on Datashares's tasks page.</p> <p>The goal of this documentation is to let you implement your own custom tasks. They could virtually be anything:</p> <ul> <li>classifying documents</li> <li>extracting named entities from documents</li> <li>extracting structured content from documents</li> <li>translating documents</li> <li>tagging documents</li> <li>...</li> </ul>"},{"location":"learn/concepts-basic/#asynchronous","title":"Asynchronous","text":"<p>In our context asynchronous mean \"executed in the background\". Since tasks can be long, getting their result is not as simple as calling an API endpoint.</p> <p>Instead, executing a task asynchronously implies:</p> <ol> <li>requesting the task execution by publish the task name and arguments (parameters needed to perform the task) on    the broker</li> <li>receive the task name and arguments from the broker and perform the actual task in the background inside    a task worker (optionally publishing progress updates on the broker)</li> <li>monitor the task progress</li> <li>saving task results or errors</li> <li>accessing the task results or errors</li> </ol>"},{"location":"learn/concepts-basic/#workers","title":"Workers","text":"<p>Workers (a.k.a. \"async apps\") are infinite loop Python programs running async tasks.</p> <p>They pseudo for the worker loop is:</p> <pre><code>while True:\n    task_name, task_args = get_next_task()\n    task_fn = get_task_function_by_name(task_name)\n    try:\n        result = task_fn(**task_args)\n    except Exception as e:\n        save_error(e)\n        continue\n    save_result(result)\n</code></pre>"},{"location":"learn/concepts-basic/#task-manager","title":"Task Manager","text":"<p>The task manager is the primary interface to interact with tasks. The task manager lets us:</p> <ul> <li>create tasks and send them to workers</li> <li>post task task state and progress updates</li> <li>monitor task state and progress</li> <li>get task results and errors</li> <li>cancel task</li> <li>...</li> </ul>"},{"location":"learn/concepts-basic/#app","title":"App","text":"<p>Apps (a.k.a. \"async apps\") are collections of tasks, they act as a registry and bind a task name to an actual unit of work (a.k.a. a Python function).</p>"},{"location":"learn/concepts-basic/#next","title":"Next","text":"<ul> <li>skip directly to learn more about tasks</li> <li>or continue to learn about advanced concepts</li> </ul>"},{"location":"learn/datashare-app/","title":"Create tasks for Datashare","text":"<p>In this section, you'll learn how to update our app to run together with Datashare.</p>"},{"location":"learn/datashare-app/#communicating-with-datashares-via-amqp","title":"Communicating with Datashare's via AMQP","text":"<p>Datashare features its own task manager used to execute many built-in tasks. This task manager can be configured in many different ways and in particular it can be configured to use AMQP via RabbitMQ to manage tasks.</p> <p>Having Datashare's task manager powered by AMQP allows us to integrate our custom tasks in Datashare's backend:</p> <ul> <li>we'll use Datashare's task manager to create custom tasks</li> <li>the task manager will publish custom tasks on the AMQP broker</li> <li>our Python worker will receive the tasks and execute them</li> <li>the workers will return their results to the task manager</li> </ul> <p>The above workflow however requires the worker to be properly configured to communicate with Datashare's task manager.</p>"},{"location":"learn/datashare-app/#aligning-workers-config-with-datashare","title":"Aligning worker's config with Datashare","text":"<p>If Datashare uses non default RabbitMQ config make sure to update your config to match Datashare's RabbitMQ config.</p> <p>Here is how to do so via the config file: app_config.json<pre><code>{\n  \"type\": \"amqp\",\n  \"rabbitmq_host\":  \"your-rabbitmq-host\",\n  \"rabbitmq_management_port\": 16666,\n  \"rabbitmq_password\": \"******\",\n  \"rabbitmq_port\": 6666,\n  \"rabbitmq_user\":  \"your-rabbitmq-user\",\n  \"rabbitmq_vhost\": \"somevhost\"\n}\n</code></pre></p> <p>If you prefer using env vars, you can update the above variables by setting the <code>ICIJ_WORKER_&lt;KEY&gt;</code> variables where<code>&lt;KEY&gt;</code> is one of the above JSON's keys. </p> <p>Read the full guide to worker config to learn more.</p>"},{"location":"learn/datashare-app/#grouping-our-tasks-in-the-python-task-group","title":"Grouping our tasks in the <code>PYTHON</code> task group","text":"<p>Inside Datashare, tasks can be grouped together in order to be executed by specific workers. In particular, Datashare defines the <code>JAVA</code> task group reserved for built-in tasks (such as indexing, batch searches/downloads...) and the <code>PYTHON</code> task group reserved for your custom tasks.</p> <p>When tasks are created and published inside Datashare by the task manager, it's possible to assign them with a group, allowing the task to be routed to specific workers.</p> <p>When creating publishing tasks with the <code>PYTHON</code> group, this will ensure only your workers will receive the tasks (and not builtin Java workers). </p> <p>If you don't use the <code>PYTHON</code> task group your worker will receive the builtin tasks reserved for Java workers, since they can't execute these tasks, they will fail.</p> <p>In order, to assign a task to a specific group, you can use the <code>group: str | TaskGroup | None</code> argument of the <code>task</code> decorator.</p> <p>Read the full guide to task routing to learn more.</p>"},{"location":"learn/datashare-app/#adding-the-mandatory-user-dict-none-argument-to-all-tasks","title":"Adding the mandatory <code>user: dict | None</code> argument to all tasks","text":"<p>Datashare systematically adds a <code>user: dict | None</code> to task arguments. Since task arguments are directly forwarded your function, they need to support the <code>user</code> argument even when used,  otherwise the task will fail complaining that the <code>user</code> argument was provided as input but is not used by the task. </p>"},{"location":"learn/datashare-app/#our-first-datashare-app","title":"Our first Datashare app","text":"<p>The app can hence be updated as following:</p> my_app.py<pre><code>from icij_worker import AsyncApp\nfrom icij_worker.app import TaskGroup\nfrom icij_worker.typing_ import RateProgress\n\napp = AsyncApp(\"some-app\")\n\nPYTHON_TASK_GROUP = TaskGroup(name=\"PYTHON\")\n\n\n@app.task(name=\"hello_world\", group=PYTHON_TASK_GROUP)\ndef hello_world(user: dict | None) -&gt; str:  # pylint: disable=unused-argument\n    return \"Hello world\"\n\n\n@app.task(name=\"hello_user\", group=PYTHON_TASK_GROUP)\ndef hello_user(user: dict | None) -&gt; str:\n    greeting = \"Hello \"\n    if user is None:\n        user = \"unknown\"\n    else:\n        user = user[\"id\"]\n    return greeting + user\n\n\n@app.task(name=\"hello_user_progress\", group=PYTHON_TASK_GROUP)\nasync def hello_user_progress(user: dict | None, progress: RateProgress) -&gt; str:\n    greeting = \"Hello \"\n    await progress(0.5)\n    if user is None:\n        user = \"unknown\"\n    else:\n        user = user[\"id\"]\n    res = greeting + user\n    await progress(1)\n    return res\n</code></pre>"},{"location":"learn/datashare-app/#running-group-specific-workers","title":"Running group-specific workers","text":"<p>As detailed in the task routing guide, worker pools can be restricted to execute tasks of a given group.</p> <p>We can hence start our <code>PYTHON</code> worker pool adding the <code>-g PYTHON</code> argument:</p> python -m icij-worker workers start -c app_config.json -g PYTHON -n 2 app.app[INFO][icij_worker.backend.backend]: Loading worker configuration from env...[INFO][icij_worker.backend.backend]: worker config: {  ...}[INFO][icij_worker.backend.mp]: starting 2 worker for app my_app.app..."},{"location":"learn/run-worker/","title":"Running a worker pool with the <code>icij-worker</code> CLI","text":"<p>In the previous section, we've created a simple async app featuring multiple tasks.</p> <p>We now need start a worker pool, in order to run the app. Workers will execute the app tasks when new tasks are published by the task manager through the broker.</p>"},{"location":"learn/run-worker/#installing-the-icij-worker-cli","title":"Installing the <code>icij-worker</code> CLI","text":"<p>Once your app is created you can use your favorite package manager to install <code>icij-worker</code>:</p> python -m pip install icij-worker"},{"location":"learn/run-worker/#start-the-worker-pool","title":"Start the worker pool","text":"<p>To start a worker pool we'll need to provide a config. Config can be provided via environment variables or using a config files. Check out our worker config guide in order to learn about all config parameters as well as env var naming conventions.  </p>"},{"location":"learn/run-worker/#using-environment-variables","title":"Using environment variables","text":"<p>Since we've put all of our app code in a variable named <code>my_app</code> in a file named <code>app.py</code>, we can start the app as following:</p> ICIJ_WORKER_TYPE=amqp python -m icij-worker workers start -n 2 app.app[INFO][icij_worker.backend.backend]: Loading worker configuration from env...[INFO][icij_worker.backend.backend]: worker config: {  ...}[INFO][icij_worker.backend.mp]: starting 2 worker for app my_app.app...  <p>Note</p> <p>The above code assumes that an AMQP broker is running on the default host and ports. Check out the worker config guide to make sure your worker can correctly connect to it.</p> <p>Let's break this down a bit <pre><code>$ ICIJ_WORKER_TYPE=amqp python -m icij-worker workers start -n 2 app.app\n</code></pre></p>"},{"location":"learn/run-worker/#using-a-config-file","title":"Using a config file","text":"<p>Sometimes it can be convenient to configure the worker pool via a config file rather than env vars, in this case you can just drop your config into a file:</p> <p>app_config.json<pre><code>{\n  \"type\": \"amqp\"\n}\n</code></pre> and start the pool like this:</p> python -m icij-worker workers start -c app_config.json -n 2 app.app[INFO][icij_worker.backend.backend]: Loading worker configuration from env...[INFO][icij_worker.backend.backend]: worker config: {  ...}[INFO][icij_worker.backend.mp]: starting 2 worker for app my_app.app..."},{"location":"learn/run-worker/#next","title":"Next","text":"<p>Here, we've built a basic app and run it using a worker pool. However, the app we've built is not able yet to speak with Datashare's Task Manager.</p> <p>Continue this tutorial to learn how implement task which can be integrated inside Datashare !</p>"},{"location":"learn/tasks/","title":"Creating asynchronous tasks","text":""},{"location":"learn/tasks/#turning-functions-into-async-tasks-using-icij-worker","title":"Turning functions into async tasks using <code>icij-worker</code>","text":"<p>Before implementing task for Datashare, let's learn how to transform any Python function into a task which can be run asynchronously by a worker using the icij-worker lib.</p> <p>Given the following function: <pre><code>def hello_world() -&gt; str:\n    return \"Hello world\"\n</code></pre></p> <p>the <code>icij-worker</code> lib lets you very simply turn it into a task executed asynchronously by a worker: </p> <pre><code>from icij_worker import AsyncApp\n\napp = AsyncApp(\"some-app\")\n\n\n@app.task\ndef hello_world() -&gt; str:\n    return \"Hello world\"\n</code></pre> <p>Let's break down the above code into pieces. First we start by defining an app and give it a name:</p> <p><pre><code>from icij_worker import AsyncApp\n\napp = AsyncApp(\"some-app\")\n\n\n@app.task\ndef hello_world() -&gt; str:\n    return \"Hello world\"\n</code></pre> the we use the <code>@app.task</code> decorator to register our <code>hello_world</code> function as a task app: <pre><code>from icij_worker import AsyncApp\n\napp = AsyncApp(\"some-app\")\n\n\n@app.task\ndef hello_world() -&gt; str:\n    return \"Hello world\"\n</code></pre></p> <p>... and that's about it, we've turned our <code>hello_world</code> very complex ML pipeline function (\ud83e\uddcc-ing) into as an async task !  </p>"},{"location":"learn/tasks/#registering-tasks-with-names","title":"Registering tasks with names","text":"<p>The <code>app</code> <code>AsyncApp</code> instance acts as task registry. By default, task are registered using the decorated function name.</p> <p>Tip</p> <p>To avoid conflicts and bugs, it's highly recommended to register tasks using a name.</p> <p>This will decouple the task name from the Python function name and avoid naming issues when you renaming/refactoing task functions.</p> <p>Applying the above tip, we can register our task with the <code>hello_world</code> name: <pre><code>from icij_worker import AsyncApp\nfrom icij_worker.typing_ import RateProgress\n\napp = AsyncApp(\"some-app\")\n\n\n@app.task(name=\"hello_world\")\ndef hello_world() -&gt; str:\n    return \"Hello world\"\n</code></pre></p>"},{"location":"learn/tasks/#task-arguments","title":"Task arguments","text":"<p>Since tasks are just plain Python function, they support input arguments.</p> <p>For instance, let's register a new task which greets a user given as a <code>str</code>. When no user is provided, let's call it <code>unknown</code>: <pre><code>@app.task(name=\"hello_user\")\ndef hello_user(user: str | None) -&gt; str:\n    greeting = \"Hello \"\n    if user is None:\n        user = \"unknown\"\n    return greeting + user\n</code></pre></p>"},{"location":"learn/tasks/#task-result","title":"Task result","text":"<p>The task result is simply the value returned by the decorated function.</p>"},{"location":"learn/tasks/#supported-argument-and-result-types","title":"Supported argument and result types","text":"<p><code>icij-worker</code> tasks support any types as arguments and result as long as they are JSON serializable.</p> <p>Tip</p> <p>If you are used to using libs such as pydantic, your task can have <code>dict</code> arguments, which you can easily convert into <code>pydantic.Model</code>. Similarly, you will have to make sure to convert output into from <code>pydantic.Model</code> to a <code>dict</code> using the <code>pydantic.Model.dict</code> method. </p>"},{"location":"learn/tasks/#publishing-progress-updates","title":"Publishing progress updates","text":"<p>For long-running task (longer than the one above), it can be useful to publish progress updates in order to monitor the task execution.</p> <p>This can be done by adding a  <code>progress: RateProgress</code> argument to task functions. The <code>progress</code> argument name is reserved and <code>icij-worker</code> will automatically populate this argument with a async coroutine that you can call to publish progress updates. Progress updates are expected to be between <code>0.0</code> and <code>1.0</code> included:</p> <pre><code>from icij_worker.typing_ import RateProgress\n\n@app.task(name=\"hello_user_progress\")\nasync def hello_user_progress(user: str | None, progress: RateProgress) -&gt; str:\n    greeting = \"Hello \"\n    await progress(0.5)\n    if user is None:\n        user = \"unknown\"\n    res = greeting + user\n    await progress(1)\n    return res\n</code></pre> <p>Note</p> <p>In practice, <code>RateProgress = Callable[[float], Awaitable[None]]</code>, which mean that publishing tasks progress is performed through an <code>asyncio</code> coroutine. To support publishing progress updates, task functions have to be async. </p> <p>Notice, how <code>hello_user_progress</code> is defined using <code>async def</code> </p>"},{"location":"learn/tasks/#next","title":"Next","text":"<p>You'll learn see what a full async app looks like !</p>"}]}